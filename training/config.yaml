# Training configuration for RetailGenius LLM fine-tuning

# Model configuration
model_name: "mistralai/Mistral-7B-Instruct-v0.2"
base_model_name: "mistralai/Mistral-7B-Instruct-v0.2"

# Dataset configuration
dataset_path: "../data/dataset.jsonl"
output_dir: "./adapters"
max_seq_length: 512

# Training hyperparameters
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
warmup_steps: 100
logging_steps: 10
save_steps: 500
eval_steps: 500

# QLoRA configuration
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules:
  - "q_proj"
  - "v_proj"
  - "k_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Evaluation configuration
eval_split: 0.1
max_eval_samples: 100

# Hardware configuration
# Note: Full training requires GPU with at least 16GB VRAM
use_fp16: true
use_4bit: true

